{
  "metadata": {
    "name": "Review",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "1. Count the number of reviews per year"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\ndf \u003d spark.table(\"review\")\r\n\r\nresult \u003d df.groupBy(date_format(col(\"rev_date\"), \"yyyy\").alias(\"Year\")) \\\r\n           .count() \\\r\n           .withColumnRenamed(\"count\", \"Review Count\") \\\r\n           .orderBy(\"Year\")\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "2. Count the number of useful, funny, and cool reviews"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\ndf \u003d spark.table(\"review\")\r\n\r\nresult \u003d df.select(\r\n               sum(\"rev_useful\").alias(\"Total Useful\"),\r\n               sum(\"rev_funny\").alias(\"Total Funny\"),\r\n               sum(\"rev_cool\").alias(\"Total Cool\")\r\n           )\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "3. Rank users by the total number of reviews each year"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nreview_df \u003d spark.table(\"review\")\r\nusers_df \u003d spark.table(\"users\")\r\n\r\nresult \u003d review_df.join(users_df, review_df.rev_user_id \u003d\u003d users_df.user_id) \\\r\n                  .groupBy(users_df.user_name.alias(\"Review User\"), \r\n                           year(col(\"rev_date\")).alias(\"Year\")) \\\r\n                  .count() \\\r\n                  .withColumnRenamed(\"count\", \"Review Count\") \\\r\n                  .orderBy(\"Year\", desc(\"Review Count\"))\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "4. Extract the Top 20 most common words from all reviews"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nfrom pyspark.sql.functions import split, explode, lower, col\r\n\r\ndf \u003d spark.table(\"review\")\r\n\r\nresult \u003d df.select(\"rev_text\") \\\r\n           .withColumn(\"word\", explode(split(lower(col(\"rev_text\")), \"\\\\W+\"))) \\\r\n           .groupBy(\"word\") \\\r\n           .count() \\\r\n           .withColumnRenamed(\"word\", \"Word\") \\\r\n           .withColumnRenamed(\"count\", \"Frequency\") \\\r\n           .orderBy(\"Frequency\", ascending\u003dFalse) \\\r\n           .limit(20)\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "5. Extract the Top 10 words from positive reviews (rating \u003e 3)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nfrom pyspark.sql.functions import split, explode, lower, col\r\n\r\ndf \u003d spark.table(\"review\")\r\n\r\nresult \u003d df.filter(col(\"rev_stars\") \u003e 3) \\\r\n           .withColumn(\"word\", explode(split(lower(col(\"rev_text\")), \"\\\\W+\"))) \\\r\n           .groupBy(\"word\") \\\r\n           .count() \\\r\n           .withColumnRenamed(\"word\", \"Word\") \\\r\n           .withColumnRenamed(\"count\", \"Frequency\") \\\r\n           .orderBy(\"Frequency\", ascending\u003dFalse) \\\r\n           .limit(10)\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "6. Extract the Top 10 words from negative reviews (rating ≤ 3)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nfrom pyspark.sql.functions import split, explode, lower, col\r\n\r\ndf \u003d spark.table(\"review\")\r\n\r\nresult \u003d df.filter(col(\"rev_stars\") \u003c\u003d 3) \\\r\n           .withColumn(\"word\", explode(split(lower(col(\"rev_text\")), \"\\\\W+\"))) \\\r\n           .groupBy(\"word\") \\\r\n           .count() \\\r\n           .withColumnRenamed(\"word\", \"Word\") \\\r\n           .withColumnRenamed(\"count\", \"Frequency\") \\\r\n           .orderBy(\"Frequency\", ascending\u003dFalse) \\\r\n           .limit(10)\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "7. Perform word cloud analysis by filtering words based on part-of-speech tagging"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nimport nltk\r\nimport os\r\nfrom pyspark.sql.functions import udf, explode, col, desc\r\nfrom pyspark.sql.types import ArrayType, StringType\r\nfrom nltk.tokenize import word_tokenize\r\nfrom nltk.corpus import stopwords\r\n\r\nnltk_data_dir \u003d os.path.expanduser(\"~/nltk_data\")\r\nnltk.data.path.insert(0, nltk_data_dir)\r\n\r\nstop_words \u003d set(stopwords.words(\u0027english\u0027))\r\n\r\n@udf(returnType\u003dArrayType(StringType()))\r\ndef extract_filtered_words(text):\r\n    words \u003d word_tokenize(text.lower())\r\n    filtered_words \u003d [word for word in words \r\n                      if word.isalpha() and len(word) \u003e 2 and word not in stop_words]\r\n    return filtered_words\r\n\r\nreviews_df \u003d spark.table(\"review\")\r\n\r\nwords_df \u003d reviews_df.withColumn(\"filtered_words\", extract_filtered_words(col(\"rev_text\")))\r\n\r\nexploded_df \u003d words_df.select(explode(col(\"filtered_words\")).alias(\"word\"))\r\n\r\nword_cloud_df \u003d exploded_df.groupBy(\"word\") \\\r\n                            .count() \\\r\n                            .withColumnRenamed(\"count\", \"frequency\") \\\r\n                            .orderBy(desc(\"frequency\"))\r\n\r\nresult \u003d word_cloud_df.withColumnRenamed(\"word\", \"Word\") \\\r\n                      .withColumnRenamed(\"frequency\", \"Frequency\")\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "8. Construct a word association graph"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nfrom pyspark.sql.functions import explode, split, lower, col, expr\r\n\r\ndf \u003d spark.table(\"review\")\r\n\r\nwords_df \u003d df.withColumn(\"words\", split(lower(col(\"rev_text\")), \"\\\\s+\"))\r\n\r\nword_pairs \u003d words_df.select(\r\n    explode(expr(\"transform(sequence(0, size(words) - 2), i -\u003e struct(words[i] as word1, words[i+1] as word2))\")).alias(\"pair\")\r\n)\r\n\r\nresult \u003d word_pairs.select(\r\n    col(\"pair.word1\").alias(\"Word 1\"),\r\n    col(\"pair.word2\").alias(\"Word 2\")\r\n).groupBy(\"Word 1\", \"Word 2\") \\\r\n .count() \\\r\n .withColumnRenamed(\"count\", \"Count\") \\\r\n .orderBy(\"Count\", ascending\u003dFalse)\r\n\r\nz.show(result)\r\n\r\n"
    }
  ]
}