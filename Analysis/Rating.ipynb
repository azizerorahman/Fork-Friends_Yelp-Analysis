{
  "metadata": {
    "name": "Rating",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "1. Analyze the distribution of ratings (1-5 stars)."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n%pyspark\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql.functions import count\r\n\r\nspark \u003d SparkSession.builder.appName(\"ReviewAnalysis\").getOrCreate()\r\n\r\nreview_df \u003d spark.table(\"review\")\r\n\r\nresult \u003d review_df.groupBy(\"rev_stars\") \\\r\n                  .agg(count(\"*\").alias(\"rating_count\")) \\\r\n                  .withColumnRenamed(\"rev_stars\", \"Stars\") \\\r\n                  .withColumnRenamed(\"rating_count\", \"Number of Ratings\") \\\r\n                  .orderBy(\"Stars\", ascending\u003dTrue)\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "2. Analyze the weekly rating frequency (Monday to Sunday)."
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nfrom pyspark.sql.functions import date_format, to_date, col, lit, create_map\r\nfrom itertools import chain\r\n\r\ndf \u003d spark.table(\"review\")\r\n\r\nday_order \u003d {\r\n    \u0027Sunday\u0027: 1,\r\n    \u0027Monday\u0027: 2,\r\n    \u0027Tuesday\u0027: 3,\r\n    \u0027Wednesday\u0027: 4,\r\n    \u0027Thursday\u0027: 5,\r\n    \u0027Friday\u0027: 6,\r\n    \u0027Saturday\u0027: 7\r\n}\r\n\r\nmapping_expr \u003d create_map([lit(x) for x in chain(*day_order.items())])\r\n\r\nresult \u003d df.groupBy(date_format(to_date(col(\"rev_date\")), \u0027EEEE\u0027).alias(\"day_of_week\")) \\\r\n           .count() \\\r\n           .withColumnRenamed(\"count\", \"Rating Count\") \\\r\n           .withColumnRenamed(\"day_of_week\", \"Day of Week\") \\\r\n           .withColumn(\"day_order\", mapping_expr[col(\"Day of Week\")]) \\\r\n           .orderBy(\"day_order\") \\\r\n           .select(\"Day of Week\", \"Rating Count\")\r\n\r\nz.show(result)\r\n\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "3. Identify the top businesses with the most five-star ratings."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n\r\n%pyspark\r\nreview_df \u003d spark.table(\"review\")\r\nbusiness_df \u003d spark.table(\"business\")\r\n\r\nresult \u003d review_df.filter(review_df.rev_stars \u003d\u003d 5) \\\r\n                  .join(business_df, review_df.rev_business_id \u003d\u003d business_df.business_id) \\\r\n                  .groupBy(business_df.name.alias(\"Business Name\")) \\\r\n                  .agg(count(\"*\").alias(\"Five Star Count\")) \\\r\n                  .orderBy(desc(\"Five Star Count\")) \\\r\n                  .limit(20)\r\n\r\nz.show(result)\r\n\r\n"
    }
  ]
}